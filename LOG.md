# Log

## 2025-12-15
### Shipped
- Day 1 Coding: Worked on Math Functions, Operators, and the data types Int and Float.

- Day 1 AI work: Worked on learning about LLMS and Prompt Engineering

### Learned
- Difference between int and float. 
- operator precedence
- checking types

- LLM's use natural language processing
- Tokens are used in GPT and they each have their own Token ID
- If asked to roll a dice, things aren't completely random. Tokens have a probability chance of following previous tokens and the highest probability can most likely be picked.
- Temperature controls randomness

### Next
- Learning Variables and Strings (str)
- Continuing through the Data Types

- Transformer Model
- Learning the training process

## 2025-12-16
### Shipped
- Day 2 Coding: Learned about Variables, Expressions vs Statements, Augmented Assignment Operator, Strings, String Concatenation, and Type Conversion
- Day 2 AI Work: Worked on looking inside LLMS and understanding parameters. Transformer Model and The Training Process were studied as well.

### Learned
- Learned the rules for naming Variables as well as showing how to indicate a constant.
- Learned the difference between Expressions and Statements. 
- Learned how to make code shorter and cleaner by using Augmented Assignment Operator. 
- Strings can be wrapped in single or double quotes.
- Long strings can be wrapped in 3 single quotes. 
- Strings and variables can be added to create outputs for them. 
- You are able to change the type of a data type by using Type Conversion.

- 300 Billion Tokens is around 45 TB of text data
- LLMs use Nueral Networks similar to our brain which use Nuerons.
- Learned about Parameters, Weights, and Bias's 
- The layers in between the input layer and output layer are known as hidden layers. Not many people know much about what happens in between. Being studied through Mechanistic Interpretability. 
- The Transformer Model allows LLMs to pay more attention.
- Long-Range Attention is able to take in more context and pay attention to words or tokens further away.
- GPT = Generative Pre-Trained Transformer
- Learned about the 2 steps to The Training Process (Pre-Training and Fine-Tuning)
- RLHF = Reinforced Learning from Human Feedback
- End of Pre-Training gives the Base Model
- End of Fine-Tuning gives the Assistant Model. 
- Learned what each step does.

### Next
- Escape Sequences, String Indexes, Immutability, and Built-In Functions + Methods
- Base Model vs. Assistant Model, The Reversal Curse, AGI